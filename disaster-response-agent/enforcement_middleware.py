"""
=============================================================================
enforcement_middleware.py â€” Programmatic Safety Enforcement Layer
Claw & Shield 2026 Hackathon | NEXUS Disaster Response Agent
=============================================================================

PURPOSE:
    This module implements "The Shield" â€” a deterministic, rule-based middleware
    that intercepts every proposed agent action BEFORE it touches the filesystem
    or any external system.

    It operates on a three-phase pipeline:
        1. PARSE   â†’ Convert raw agent output into a typed IntentModel
        2. EVALUATE â†’ Run the IntentModel against a PolicyModel (rule engine)
        3. ENFORCE  â†’ Allow or raise PolicyViolationError (blocks execution)

    Critically, enforcement is NOT delegated to the LLM. The rules are:
        - Mathematically expressed as set-intersection checks (O(n) term scans)
        - Path-resolved using Python's pathlib (prevents directory traversal)
        - Deterministic and side-effect-free (pure functions only)

WHY THIS MATTERS FOR JUDGES:
    Prompt engineering ("don't do X") is probabilistic â€” the model can comply
    or fail depending on temperature, context drift, or jailbreaks. This layer
    enforces policy in compiled Python, which is immune to such failure modes.

Author: NEXUS Team â€” Claw & Shield 2026
=============================================================================
"""

# =============================================================================
# IMPORTS
# =============================================================================
from __future__ import annotations

import re
import logging
from dataclasses import dataclass, field
from enum import Enum, auto
from pathlib import Path
from typing import FrozenSet, List, Optional, Set

# =============================================================================
# LOGGING
# =============================================================================
logger = logging.getLogger("NEXUS_SHIELD")


# =============================================================================
# EXCEPTIONS
# =============================================================================

class PolicyViolationError(RuntimeError):
    """
    Raised when an agent's proposed intent violates the active PolicyModel.

    This is a hard, non-recoverable error at the enforcement layer.
    The agent runtime MUST catch this and halt the action â€” it must NOT
    retry, rephrase, or attempt an alternative tool call.

    Attributes:
        reason (str):  Human-readable violation description (shown in terminal).
        rule_id (str): Machine-readable rule identifier for audit logging.
    """
    def __init__(self, reason: str, rule_id: str = "UNSPECIFIED") -> None:
        self.reason = reason
        self.rule_id = rule_id
        super().__init__(
            f"\n{'='*70}\n"
            f"  ðŸ›‘  POLICY VIOLATION â€” ACTION BLOCKED BY THE SHIELD\n"
            f"{'='*70}\n"
            f"  Rule ID : {rule_id}\n"
            f"  Reason  : {reason}\n"
            f"{'='*70}\n"
        )


# =============================================================================
# ENUMS
# =============================================================================

class ActionType(Enum):
    """
    Taxonomy of all actions that the agent is allowed to propose.
    Extending this enum is the ONLY way to introduce a new action class â€”
    preventing "surprise" actions from slipping through the middleware.
    """
    WRITE_DISPATCH_LOG  = auto()   # Write a logistics JSON to outgoing_dispatch
    READ_RESOURCE       = auto()   # Read a file (future: read situation reports)
    SEND_NOTIFICATION   = auto()   # Future: SMS / webhook dispatch
    UNKNOWN             = auto()   # Catch-all for unrecognized proposals


class DisasterCategory(Enum):
    """Accepted top-level emergency categories for logistics dispatch."""
    FLOOD          = "flood"
    EARTHQUAKE     = "earthquake"
    WILDFIRE       = "wildfire"
    CYCLONE        = "cyclone"
    INFRASTRUCTURE = "infrastructure"
    EVACUATION     = "evacuation"
    SEARCH_RESCUE  = "search_rescue"
    LOGISTICS      = "logistics"
    UNKNOWN        = "unknown"


# =============================================================================
# DATA MODELS
# =============================================================================

@dataclass
class IntentModel:
    """
    Typed representation of the agent's PARSED INTENT for a single action.

    This is the canonical intermediate representation (IR) that the Shield
    operates on. The AgentCore must populate this from the LLM's structured
    output BEFORE calling enforce().

    Fields:
        action_type       : What category of action is being proposed.
        raw_text          : The original unmodified text the LLM produced.
        proposed_filepath : The absolute Path the agent wants to write to.
        disaster_category : The categorized emergency type.
        keywords          : Bag-of-words extracted from the LLM's reasoning.
        metadata          : Arbitrary key-value pairs from the agent's response.
    """
    action_type:        ActionType
    raw_text:           str
    proposed_filepath:  Optional[Path]             = None
    disaster_category:  DisasterCategory            = DisasterCategory.UNKNOWN
    keywords:           FrozenSet[str]              = field(default_factory=frozenset)
    metadata:           dict                        = field(default_factory=dict)

    def __post_init__(self) -> None:
        # Normalize the keywords to lowercase for case-insensitive matching
        if self.keywords:
            object.__setattr__(
                self,
                "keywords",
                frozenset(k.lower().strip() for k in self.keywords)
            )


@dataclass(frozen=True)
class PolicyModel:
    """
    Immutable rule definition that specifies WHAT IS ALLOWED.

    Every field is a declarative constraint. The Shield evaluates IntentModels
    against this model using pure intersection / containment logic.

    Fields:
        allowed_action_types    : Set of ActionType values that are permitted.
        allowed_base_directory  : Only writes to this directory are permitted.
        blocked_keyword_sets    : List of keyword sets â€” if ANY set is fully
                                  present in the intent's keywords OR raw text,
                                  the action is blocked. (AND-within-set logic,
                                  OR-across-sets logic.)
        blocked_regex_patterns  : Additional regex patterns to block on raw text.
        max_filepath_depth      : How many subdirectories deep relative to base
                                  the agent is allowed to write. (1 = base only)
        allow_subdirectories    : If False, only direct children of base dir.
    """
    allowed_action_types:   FrozenSet[ActionType]       = field(
        default_factory=lambda: frozenset({ActionType.WRITE_DISPATCH_LOG})
    )
    allowed_base_directory: Path                        = field(
        default_factory=lambda: Path("/app/workspace/outgoing_dispatch").resolve()
    )
    blocked_keyword_sets:   tuple                       = field(
        default_factory=lambda: (
            # â”€â”€ MEDICAL DIAGNOSIS / TREATMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            frozenset({"diagnosis", "treatment"}),
            frozenset({"prescription", "medication"}),
            frozenset({"medical", "advice"}),
            frozenset({"triage", "injury", "wound"}),
            frozenset({"burns", "laceration", "fracture"}),
            frozenset({"surgery", "procedure", "anesthesia"}),
            frozenset({"drug", "dosage", "antibiotic"}),
            frozenset({"patient", "clinical", "symptom"}),
            frozenset({"diagnose", "treat", "prescribe"}),
            frozenset({"therapy", "rehabilitation"}),
            frozenset({"infection", "sterile", "suture"}),
        )
    )
    blocked_regex_patterns: tuple                       = field(
        default_factory=lambda: (
            # Catch "treat the patient", "prescribe X mg", "diagnose Y", etc.
            r"\b(treat|treating|treated)\b.{0,50}\b(patient|victim|casualty)\b",
            r"\b(prescribe|prescription)\b",
            r"\b(diagnos[ei][sd]?)\b",
            r"\b(medical\s+advice|clinical\s+assessment|clinical\s+diagnosis)\b",
            r"\b(mg|ml|tablet|capsule|injection|iv\s+drip)\b",
            r"\b(burn\s+wound|second[\s\-]degree|third[\s\-]degree)\b",
        )
    )
    max_filepath_depth:     int                         = 1
    allow_subdirectories:   bool                        = False


# =============================================================================
# CORE ENFORCEMENT FUNCTIONS
# =============================================================================

def _check_medical_keywords(
    intent:  IntentModel,
    policy:  PolicyModel,
) -> Optional[str]:
    """
    Phase 1 check: Scan intent.keywords AND intent.raw_text for blocked terms.

    Strategy:
        - Tokenize raw_text into a bag-of-words (alphanumeric tokens, lowercase)
        - Merge with intent.keywords for a unified token set
        - For each blocked_keyword_set in the policy:
              if blocked_keyword_set âŠ† unified_tokens â†’ BLOCK
        - Additionally, run each regex pattern against the raw_text

    Returns:
        str  â€” a violation reason string if blocked, else None (allow).
    """
    # Build unified token bag from both keywords field and raw text
    raw_tokens: Set[str] = set(
        re.findall(r"[a-z]+", intent.raw_text.lower())
    )
    unified_tokens: Set[str] = raw_tokens | set(intent.keywords)

    # --- Keyword-set intersection checks ------------------------------------
    for blocked_set in policy.blocked_keyword_sets:
        intersection = blocked_set & unified_tokens
        if intersection == blocked_set:
            return (
                f"Medical/out-of-scope terminology detected. "
                f"Blocked keyword cluster: {sorted(blocked_set)}"
            )

    # --- Regex pattern checks -----------------------------------------------
    for pattern in policy.blocked_regex_patterns:
        match = re.search(pattern, intent.raw_text, re.IGNORECASE)
        if match:
            return (
                f"Blocked regex pattern matched in intent text. "
                f"Pattern: `{pattern}` | Match: `{match.group(0)}`"
            )

    return None   # âœ… No medical/blocked content found


def _check_filepath_scope(
    intent:  IntentModel,
    policy:  PolicyModel,
) -> Optional[str]:
    """
    Phase 2 check: Validate that the proposed file path is within policy scope.

    Strategy:
        - Resolve proposed_filepath to an absolute path (removes `../` traversal)
        - Check that it starts with allowed_base_directory (containment check)
        - Check that depth relative to base does not exceed max_filepath_depth
        - If allow_subdirectories is False, enforce direct-child-only rule

    Returns:
        str  â€” a violation reason string if blocked, else None (allow).
    """
    if intent.proposed_filepath is None:
        # No file operation requested; path check is not applicable
        return None

    resolved: Path = intent.proposed_filepath.resolve()
    base:     Path = policy.allowed_base_directory

    # Containment check â€” prevents directory traversal attacks
    try:
        relative = resolved.relative_to(base)
    except ValueError:
        return (
            f"Directory scope violation: proposed path `{resolved}` "
            f"is outside the allowed base `{base}`."
        )

    # Depth check
    depth = len(relative.parts)
    if depth > policy.max_filepath_depth:
        return (
            f"File depth violation: path is {depth} level(s) deep, "
            f"but policy allows max {policy.max_filepath_depth}."
        )

    # Subdirectory check
    if not policy.allow_subdirectories and depth > 1:
        return (
            f"Subdirectory violation: policy requires files to be direct "
            f"children of `{base}`, but `{resolved}` has subdirectory nesting."
        )

    return None   # âœ… Path is within scope


def _check_action_type(
    intent:  IntentModel,
    policy:  PolicyModel,
) -> Optional[str]:
    """
    Phase 3 check: Verify that the proposed action type is permitted.

    Returns:
        str  â€” a violation reason string if blocked, else None (allow).
    """
    if intent.action_type not in policy.allowed_action_types:
        allowed = [a.name for a in policy.allowed_action_types]
        return (
            f"Action type `{intent.action_type.name}` is not permitted. "
            f"Allowed types: {allowed}"
        )
    return None   # âœ… Action type is permitted


# =============================================================================
# PUBLIC INTERFACE â€” The Shield
# =============================================================================

def enforce(intent: IntentModel, policy: PolicyModel) -> None:
    """
    THE SHIELD â€” Master enforcement entry point.

    Runs all policy checks sequentially. Raises PolicyViolationError at the
    FIRST violation detected (fail-fast semantics). If all checks pass,
    this function returns None, signalling the agent runtime to proceed.

    Evaluation order (most specific to most general):
        1. Action type allowlist
        2. Medical / out-of-scope keyword detection  â† "The Medical Block"
        3. Filesystem path scope validation           â† "The Directory Lock"

    Args:
        intent : The parsed IntentModel from the agent's proposed action.
        policy : The active PolicyModel defining what is permitted.

    Raises:
        PolicyViolationError: If any check fails. The error message is
                              designed for terminal visibility and audit logs.

    Returns:
        None â€” If (and only if) ALL checks pass.
    """
    logger.info(
        "ðŸ›¡ï¸  SHIELD EVALUATING: action=%s | file=%s | keywords=%s",
        intent.action_type.name,
        intent.proposed_filepath,
        list(intent.keywords)[:8],   # show at most 8 to avoid log flooding
    )

    # â”€â”€ Check 1: Action Type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    violation = _check_action_type(intent, policy)
    if violation:
        logger.critical("ðŸ›‘ SHIELD BLOCK [RULE:ACTION_TYPE] %s", violation)
        raise PolicyViolationError(reason=violation, rule_id="RULE:ACTION_TYPE")

    # â”€â”€ Check 2: Medical / Blocked Content â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    violation = _check_medical_keywords(intent, policy)
    if violation:
        logger.critical("ðŸ›‘ SHIELD BLOCK [RULE:MEDICAL_BLOCK] %s", violation)
        raise PolicyViolationError(reason=violation, rule_id="RULE:MEDICAL_BLOCK")

    # â”€â”€ Check 3: Filesystem Scope â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    violation = _check_filepath_scope(intent, policy)
    if violation:
        logger.critical("ðŸ›‘ SHIELD BLOCK [RULE:DIR_SCOPE] %s", violation)
        raise PolicyViolationError(reason=violation, rule_id="RULE:DIR_SCOPE")

    # â”€â”€ All checks passed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("âœ… SHIELD CLEARED: action approved for execution")
    return None


# =============================================================================
# UTILITY â€” Intent Extraction Helper
# =============================================================================

# Medical-related terms to scan for during intent extraction
_MEDICAL_SIGNALS: FrozenSet[str] = frozenset({
    "diagnosis", "treatment", "prescription", "medication", "medical",
    "triage", "wound", "burn", "fracture", "surgery", "drug", "dosage",
    "patient", "symptom", "therapy", "infection", "antibiotic", "clinical",
    "injury", "injuries", "casualties", "treat", "prescribe", "diagnose",
    "rehabilitate", "anesthesia", "suture", "laceration", "sterile",
})

# Logistics-related terms that map to valid disaster categories
_LOGISTICS_SIGNALS: dict = {
    "flood":          DisasterCategory.FLOOD,
    "flooding":       DisasterCategory.FLOOD,
    "earthquake":     DisasterCategory.EARTHQUAKE,
    "seismic":        DisasterCategory.EARTHQUAKE,
    "wildfire":       DisasterCategory.WILDFIRE,
    "fire":           DisasterCategory.WILDFIRE,
    "cyclone":        DisasterCategory.CYCLONE,
    "hurricane":      DisasterCategory.CYCLONE,
    "typhoon":        DisasterCategory.CYCLONE,
    "evacuation":     DisasterCategory.EVACUATION,
    "evacuate":       DisasterCategory.EVACUATION,
    "rescue":         DisasterCategory.SEARCH_RESCUE,
    "search":         DisasterCategory.SEARCH_RESCUE,
    "logistics":      DisasterCategory.LOGISTICS,
    "dispatch":       DisasterCategory.LOGISTICS,
    "infrastructure": DisasterCategory.INFRASTRUCTURE,
    "bridge":         DisasterCategory.INFRASTRUCTURE,
    "road":           DisasterCategory.INFRASTRUCTURE,
}


def extract_intent_from_prompt(
    raw_text: str,
    proposed_filepath: Optional[Path] = None,
) -> IntentModel:
    """
    Lightweight intent extractor that converts a raw user/LLM prompt into
    a typed IntentModel WITHOUT calling the LLM again.

    This is intentionally a deterministic function (regex + dict lookup)
    so that enforcement does not depend on a second LLM call, which could
    itself be manipulated.

    Args:
        raw_text          : The raw text prompt / LLM reasoning to analyze.
        proposed_filepath : The file path the agent proposes to write to.

    Returns:
        IntentModel with extracted fields populated.
    """
    tokens: Set[str] = set(re.findall(r"[a-z]+", raw_text.lower()))

    # Extract keyword signals for the policy engine
    extracted_keywords = tokens & (_MEDICAL_SIGNALS | set(_LOGISTICS_SIGNALS.keys()))

    # Determine disaster category from the first match found
    category = DisasterCategory.UNKNOWN
    for token in tokens:
        if token in _LOGISTICS_SIGNALS:
            category = _LOGISTICS_SIGNALS[token]
            break

    return IntentModel(
        action_type=ActionType.WRITE_DISPATCH_LOG,
        raw_text=raw_text,
        proposed_filepath=proposed_filepath,
        disaster_category=category,
        keywords=frozenset(extracted_keywords),
    )
